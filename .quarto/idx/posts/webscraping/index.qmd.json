{"title":"Scraping web data with R and Docker","markdown":{"yaml":{"title":"Scraping web data with R and Docker","author":"Milt","date":"2020-10-28","categories":["Data Science/Programming"]},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nI've spent a considerable amount of time sifting through tutorials on how to scrape data from the web. Both R and Python offer tools to easily parse html data but so far the only easy solution I've found to scrape dynamic data rendered though JS (maybe it's JS?, either way it shows up magicially on the screen but isn't in the html to parse) is in R.\n\nFor this little project we will be recording the building capacity counts for the UW Madison Rec. Inside this site there are two pages we'll want to scrape. The first page is the overall building capactiy at `https://services.recwell.wisc.edu/FacilityOccupancy` and a more granular look at specific area capactiy in `https://recwell.wisc.edu/liveusage/`.\n\nLastly, I don't remember what I've installed onto my computer to make this all run, but you don't need to worry about that because we'll also put together a docker image you can build to easily run both of the examples below.\n\n### Using rvest (EASY MODE)\n\nFirst we'll start with scraping overall building occupancy. For this we just need to grab the HTML from the URL `https://services.recwell.wisc.edu/FacilityOccupancy` and strip out the numbers that we want using their xpaths.\n\n``` r\nlibrary(rvest)\nlibrary(tidyverse)\nurlPath <- 'https://services.recwell.wisc.edu/FacilityOccupancy'\nhtml <- read_html(urlPath) \n\n#output of html\n> html\n{html_document}  \n<html lang=\"en-US\">\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\\n<link rel=\"icon\" href=\"~/favicon.ico\">\\n<meta charset=\"utf-8\">\\n<meta name=\"v \n[2] <body>\\r\\n    <div role=\"complementary\" aria-label=\"skip to main content\"><a id=\"skipLink\" class=\"skip-main\" tabindex=\"1\" onclick=\"$('#mainContent').find( \n```\n\nNow we have a list of html text stored in the variable `html` (output shown above). Inside of `html` are the two numbers we want to extract: Current Occupancy and max Occupancy. First you need to find the xpaths (or something similiar) for each occupancy value you want to pull out. If you don't know how to do this just search for 'finding xpath in chrome' and you're sure to find something. Once you have the xpath for the values you want the rest is pretty straightforward.\n\n``` r\noccupancy <- \n  html %>%\n  html_nodes(xpath='//*[@id=\"occupancy-65cc2f42-1ca8-4afe-bf5a-990b1a9e4111\"]/div[2]/p[3]/strong | //*[@id=\"occupancy-65cc2f42-1ca8-4afe-bf5a-990b1a9e4111\"]/div[2]/p[1]/strong') %>%\n  html_text() %>% \n  str_replace(\"%\",\"\") %>% \n  as.numeric() \n  \n> occupancy\n[1] 222  73\n```\n\nIn the code above, `html_nodes()` will extract the values we want from the html based on the xpaths we gave it to look for. This value will still have html tags on it (e.g. \\<strong\\> 222 \\</strong\\>) which we can remove using the `html_text()` function. From there it's just some simple cleanup to remove special characters with `str_replace()` and convert them from character into numeric using `as.numeric()`.\n\nFinally we can take our vector and turn it into a dataframe for additional manipulation.\n\n``` r\ndf <- data.frame(max_occupancy  = occupancy[1], current_occupancy = occupancy[2]/100, pulled = Sys.time())\n> df\n  max_occupancy current_occupancy              pulled\n1           222              0.73 2020-10-28 19:35:19\n```\n\n### Using pagedown and pdftools\n\nOK, now lets wrap this up and head over to `https://recwell.wisc.edu/liveusage/`, grab a few xpaths and be good to go. Unfortunatly this one isn't quite as easy.\n\n``` r\nurlPath <- 'https://recwell.wisc.edu/liveusage/'\nhtml <- read_html(urlPath)\n\nhtml %>%\nhtml_nodes(xpath='//*[@id=\"nick\"]/div/div[2]/div[2]/div/div/div[1]/div/div[2]/p[2]/span[1]') %>%\n  html_text()\n  \n[1] \"0\"\n```\n\nThe code above will return 0 instead of 18 which is the correct value at the time I ran this example. I believe 0 is a placeholder and the actual value gets updated at a later time. Either way, it doesn't look like the method we used above will work for us so we'll have to shift gears a bit.\n\nThe most reliable way I've found to scrape this type of data is to use `pagedown` to print the page to a pdf. Then use `pdftools` to read the pdf back into R. Then use some regular expression magic to parse everything you need.\n\n``` r\n#Set some variables\nlibrary(pdftools)\nlibrary(tidyverse)\nurlPath <- 'https://recwell.wisc.edu/liveusage/'\npdfPath <- \"./rec.pdf\"\n\n#Print out the webpage to pdf using pagedown\npagedown::chrome_print(urlPath,pdfPath,extra_args = '--no-sandbox')\n\n#Read in the pdf again using pdftools (and some stringr magic)\nx <- str_squish(unlist(str_split(str_flatten(pdf_text(pdfPath)),\"\\n\")))\n\n#Use regular expressions to parse out the data we want to keep\nvalues_bool <- str_detect(x,\"^Updated (.*?) \\\\d+ / \\\\d+\")\nlabel_bool <- values_bool[c(2:length(values_bool),FALSE)]\n\n#Turn it into a dataframe for additional manipulation.\ndata.frame(location = x[label_bool],values = x[values_bool])\n```\n\n### Making it easy with Docker\n\nYou can build your own image using the Dockerfile code below, or pull mine from docker hub at mjholt02/pagedown.\n\n``` r\nFROM r-base\n\nRUN apt-get update -qq && apt-get -y install libssl-dev \\\n    chromium libcurl4-openssl-dev \\ \n    libxml2-dev libpoppler-cpp-dev libpq-dev\n\nRUN install2.r RPostgres pagedown pdftools tidyverse \\ \n    rvest\n```\n\nFrom there is as easy as starting up an interactive container using `docker run --rm -it mjholt02/pagedown` or `winpty docker run --rm -it mjholt02/pagedown` if you're using git bash.\n","srcMarkdownNoYaml":"\n\n### Introduction\n\nI've spent a considerable amount of time sifting through tutorials on how to scrape data from the web. Both R and Python offer tools to easily parse html data but so far the only easy solution I've found to scrape dynamic data rendered though JS (maybe it's JS?, either way it shows up magicially on the screen but isn't in the html to parse) is in R.\n\nFor this little project we will be recording the building capacity counts for the UW Madison Rec. Inside this site there are two pages we'll want to scrape. The first page is the overall building capactiy at `https://services.recwell.wisc.edu/FacilityOccupancy` and a more granular look at specific area capactiy in `https://recwell.wisc.edu/liveusage/`.\n\nLastly, I don't remember what I've installed onto my computer to make this all run, but you don't need to worry about that because we'll also put together a docker image you can build to easily run both of the examples below.\n\n### Using rvest (EASY MODE)\n\nFirst we'll start with scraping overall building occupancy. For this we just need to grab the HTML from the URL `https://services.recwell.wisc.edu/FacilityOccupancy` and strip out the numbers that we want using their xpaths.\n\n``` r\nlibrary(rvest)\nlibrary(tidyverse)\nurlPath <- 'https://services.recwell.wisc.edu/FacilityOccupancy'\nhtml <- read_html(urlPath) \n\n#output of html\n> html\n{html_document}  \n<html lang=\"en-US\">\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\\n<link rel=\"icon\" href=\"~/favicon.ico\">\\n<meta charset=\"utf-8\">\\n<meta name=\"v \n[2] <body>\\r\\n    <div role=\"complementary\" aria-label=\"skip to main content\"><a id=\"skipLink\" class=\"skip-main\" tabindex=\"1\" onclick=\"$('#mainContent').find( \n```\n\nNow we have a list of html text stored in the variable `html` (output shown above). Inside of `html` are the two numbers we want to extract: Current Occupancy and max Occupancy. First you need to find the xpaths (or something similiar) for each occupancy value you want to pull out. If you don't know how to do this just search for 'finding xpath in chrome' and you're sure to find something. Once you have the xpath for the values you want the rest is pretty straightforward.\n\n``` r\noccupancy <- \n  html %>%\n  html_nodes(xpath='//*[@id=\"occupancy-65cc2f42-1ca8-4afe-bf5a-990b1a9e4111\"]/div[2]/p[3]/strong | //*[@id=\"occupancy-65cc2f42-1ca8-4afe-bf5a-990b1a9e4111\"]/div[2]/p[1]/strong') %>%\n  html_text() %>% \n  str_replace(\"%\",\"\") %>% \n  as.numeric() \n  \n> occupancy\n[1] 222  73\n```\n\nIn the code above, `html_nodes()` will extract the values we want from the html based on the xpaths we gave it to look for. This value will still have html tags on it (e.g. \\<strong\\> 222 \\</strong\\>) which we can remove using the `html_text()` function. From there it's just some simple cleanup to remove special characters with `str_replace()` and convert them from character into numeric using `as.numeric()`.\n\nFinally we can take our vector and turn it into a dataframe for additional manipulation.\n\n``` r\ndf <- data.frame(max_occupancy  = occupancy[1], current_occupancy = occupancy[2]/100, pulled = Sys.time())\n> df\n  max_occupancy current_occupancy              pulled\n1           222              0.73 2020-10-28 19:35:19\n```\n\n### Using pagedown and pdftools\n\nOK, now lets wrap this up and head over to `https://recwell.wisc.edu/liveusage/`, grab a few xpaths and be good to go. Unfortunatly this one isn't quite as easy.\n\n``` r\nurlPath <- 'https://recwell.wisc.edu/liveusage/'\nhtml <- read_html(urlPath)\n\nhtml %>%\nhtml_nodes(xpath='//*[@id=\"nick\"]/div/div[2]/div[2]/div/div/div[1]/div/div[2]/p[2]/span[1]') %>%\n  html_text()\n  \n[1] \"0\"\n```\n\nThe code above will return 0 instead of 18 which is the correct value at the time I ran this example. I believe 0 is a placeholder and the actual value gets updated at a later time. Either way, it doesn't look like the method we used above will work for us so we'll have to shift gears a bit.\n\nThe most reliable way I've found to scrape this type of data is to use `pagedown` to print the page to a pdf. Then use `pdftools` to read the pdf back into R. Then use some regular expression magic to parse everything you need.\n\n``` r\n#Set some variables\nlibrary(pdftools)\nlibrary(tidyverse)\nurlPath <- 'https://recwell.wisc.edu/liveusage/'\npdfPath <- \"./rec.pdf\"\n\n#Print out the webpage to pdf using pagedown\npagedown::chrome_print(urlPath,pdfPath,extra_args = '--no-sandbox')\n\n#Read in the pdf again using pdftools (and some stringr magic)\nx <- str_squish(unlist(str_split(str_flatten(pdf_text(pdfPath)),\"\\n\")))\n\n#Use regular expressions to parse out the data we want to keep\nvalues_bool <- str_detect(x,\"^Updated (.*?) \\\\d+ / \\\\d+\")\nlabel_bool <- values_bool[c(2:length(values_bool),FALSE)]\n\n#Turn it into a dataframe for additional manipulation.\ndata.frame(location = x[label_bool],values = x[values_bool])\n```\n\n### Making it easy with Docker\n\nYou can build your own image using the Dockerfile code below, or pull mine from docker hub at mjholt02/pagedown.\n\n``` r\nFROM r-base\n\nRUN apt-get update -qq && apt-get -y install libssl-dev \\\n    chromium libcurl4-openssl-dev \\ \n    libxml2-dev libpoppler-cpp-dev libpq-dev\n\nRUN install2.r RPostgres pagedown pdftools tidyverse \\ \n    rvest\n```\n\nFrom there is as easy as starting up an interactive container using `docker run --rm -it mjholt02/pagedown` or `winpty docker run --rm -it mjholt02/pagedown` if you're using git bash.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"highlight-style":"eiffel","output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","editor":"visual","theme":{"dark":"darkly","light":"cosmo"},"title-block-banner":true,"title":"Scraping web data with R and Docker","author":"Milt","date":"2020-10-28","categories":["Data Science/Programming"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}